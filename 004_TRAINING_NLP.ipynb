{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "004_Applying_Modern_DeepLearning_NLP",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iIGjBVxfs_m",
        "colab_type": "text"
      },
      "source": [
        "build and train deep learning architectures, such as RNN, for an NLP task. The task at hand is emotion classification which is a multi-class problem.\n",
        "\n",
        "---\n",
        "\n",
        "### Steps:\n",
        "\n",
        "- Load the Data\n",
        "- Implementing Model\n",
        "- Pretesting Model\n",
        "- Setup Training\n",
        "- Traing Model\n",
        "- Storing Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpKxFsAqqE0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6mdHUdLdHTf",
        "colab_type": "text"
      },
      "source": [
        "### Load the Data\n",
        "Instead of reloading the data, we restore it from the previous phase.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BBLZ0sFdE9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# helper functions\n",
        "def convert_to_pickle(item, directory):\n",
        "    pickle.dump(item, open(directory,\"wb\"))\n",
        "\n",
        "\n",
        "def load_from_pickle(directory):\n",
        "    return pickle.load(open(directory,\"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1pzre8l6-u",
        "colab_type": "code",
        "outputId": "2e678676-8247-4fb2-ffad-79f364c2e8e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# READ YOUR DATA FROM GOOGLE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKt8eof-IqgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data instance\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x, y, x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtvrtPifs537",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = \"/gdrive/My Drive/NLP_PyTorch/\"\n",
        "\n",
        "train_dataset = load_from_pickle(data_folder + \"train_dataset\")\n",
        "test_dataset = load_from_pickle(data_folder + \"test_dataset\")\n",
        "val_dataset = load_from_pickle(data_folder + \"val_dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtHq06XGJkV4",
        "colab_type": "code",
        "outputId": "63cf0a4f-af60-452a-93c5-791cfee44f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_dataset.batch_size"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP3VrH2h5Q7C",
        "colab_type": "text"
      },
      "source": [
        "### Implementing Model\n",
        "\n",
        "After the data has been preprocessed, transformed and prepared it is now time to construct the model or the so-called computation graph that will be used to train our classification models. We are going to use a gated recurrent neural network (GRU), which is considered a more efficient version of a basic RNN. The figure below shows a high-level overview of the model details. \n",
        "\n",
        "![alt txt](https://github.com/omarsar/nlp_pytorch_tensorflow_notebooks/blob/master/img/gru-model.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXwdHHeQfugz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmoGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
        "        super(EmoGRU, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # layers\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_units)\n",
        "        self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
        "    \n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.hidden_units)).to(device)\n",
        "    \n",
        "    def forward(self, x, lens, device):\n",
        "        x = self.embedding(x)\n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        output, self.hidden = self.gru(x, self.hidden) # max_len X batch_size X hidden_units\n",
        "        out = output[-1, :, :] \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out, self.hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzcFnWB07Ev2",
        "colab_type": "text"
      },
      "source": [
        "### Pretesting models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqSmsXuoKDM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "TRAIN_BUFFER_SIZE = 40000 # len(input_tensor_train)\n",
        "VAL_BUFFER_SIZE = 5000 # len(input_tensor_val)\n",
        "TEST_BUFFER_SIZE = 5000 # len(input_tensor_test)\n",
        "BATCH_SIZE = 64\n",
        "TRAIN_N_BATCH = TRAIN_BUFFER_SIZE // BATCH_SIZE\n",
        "VAL_N_BATCH = VAL_BUFFER_SIZE // BATCH_SIZE\n",
        "TEST_N_BATCH = TEST_BUFFER_SIZE // BATCH_SIZE\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = 27291 # len(inputs.word2idx)\n",
        "target_size = 6 # num_emotions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CekF-k7m7F_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort batch function to be able to use with pad_packed_sequence        \n",
        "# batch elements ordered decreasingle by their length\n",
        "\n",
        "def sort_batch(X, y, lengths):                                     # made for machine translation task, not need for classification, but very useful\n",
        "    \"sort the batch by length\"\n",
        "    \n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFmJQKBzdn3F",
        "colab_type": "text"
      },
      "source": [
        "`pad_packed_sequence` is a utility function to efficiently and automatically pad your data of variable length sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WziHVOpG7Lcz",
        "colab_type": "code",
        "outputId": "f84ec6f0-f96e-4229-84d8-ad0e11babc35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
        "model.to(device)\n",
        "\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(train_dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "print(\"Input size: \", xs.size())\n",
        "\n",
        "output, _ = model(xs.to(device), lens, device)\n",
        "print(output.size())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input size:  torch.Size([69, 64])\n",
            "torch.Size([64, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUeoDIDX7Ovk",
        "colab_type": "text"
      },
      "source": [
        "### Setup Training\n",
        "Now that we have tested the model, it is time to train it. We will define out optimization algorithm, learnin rate, and other necessary information to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeuPVVgw7SZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enabling cuda\n",
        "use_cuda = True if torch.cuda.is_available() else False\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
        "model.to(device)\n",
        "\n",
        "# loss criterion and optimizer for training\n",
        "criterion = nn.CrossEntropyLoss() # the same as log_softmax + NLLLoss\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "def loss_function(y, prediction):\n",
        "    \"\"\" CrossEntropyLoss expects outputs and class indices as target \"\"\"\n",
        "    # convert from one-hot encoding to class indices\n",
        "    target = torch.max(y, 1)[1]\n",
        "    loss = criterion(prediction, target) \n",
        "    return loss   #TODO: refer the parameter of these functions as the same\n",
        "    \n",
        "def accuracy(target, logit):\n",
        "    ''' Obtain accuracy for training round '''\n",
        "    target = torch.max(target, 1)[1] # convert from one-hot encoding to class indices\n",
        "    corrects = (torch.max(logit, 1)[1].data == target).sum()\n",
        "    accuracy = 100.0 * corrects / len(logit)\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc3tkiqQ9JzH",
        "colab_type": "text"
      },
      "source": [
        "### Training Model\n",
        "\n",
        "Now we finally train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujawKs7x9Lc8",
        "colab_type": "code",
        "outputId": "6c54542c-cc2e-4a0b-8092-ff34e86ef763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    ### Initialize hidden state\n",
        "    # TODO: do initialization here.\n",
        "    total_loss = 0\n",
        "    train_accuracy, val_accuracy = 0, 0\n",
        "    \n",
        "    ### Training\n",
        "    for (batch, (inp, targ, lens)) in enumerate(train_dataset):\n",
        "        loss = 0\n",
        "        predictions, _ = model(inp.permute(1 ,0).to(device), lens, device) # TODO:don't need _   \n",
        "              \n",
        "        loss += loss_function(targ.to(device), predictions)\n",
        "        batch_loss = (loss / int(targ.shape[1]))        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()                     # standard code in PyTorch model\n",
        "        loss.backward()                           # standard code in PyTorch model\n",
        "        optimizer.step()                          # standard code in PyTorch model\n",
        "        \n",
        "        batch_accuracy = accuracy(targ.to(device), predictions)\n",
        "        train_accuracy += batch_accuracy\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Val. Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.cpu().detach().numpy()))\n",
        "            \n",
        "    ### Validating\n",
        "    for (batch, (inp, targ, lens)) in enumerate(val_dataset):        \n",
        "        predictions,_ = model(inp.permute(1, 0).to(device), lens, device)        \n",
        "        batch_accuracy = accuracy(targ.to(device), predictions)\n",
        "        val_accuracy += batch_accuracy\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1, \n",
        "                                                             total_loss / TRAIN_N_BATCH, \n",
        "                                                             train_accuracy / TRAIN_N_BATCH,\n",
        "                                                             val_accuracy / VAL_N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Val. Loss 0.3010\n",
            "Epoch 1 Batch 100 Val. Loss 0.2606\n",
            "Epoch 1 Batch 200 Val. Loss 0.2146\n",
            "Epoch 1 Batch 300 Val. Loss 0.1203\n",
            "Epoch 1 Batch 400 Val. Loss 0.0841\n",
            "Epoch 1 Batch 500 Val. Loss 0.0365\n",
            "Epoch 1 Batch 600 Val. Loss 0.0399\n",
            "Epoch 1 Loss 0.1420 -- Train Acc. 67.2050 -- Val Acc. 91.8670\n",
            "Time taken for 1 epoch 31.093199253082275 sec\n",
            "\n",
            "Epoch 2 Batch 0 Val. Loss 0.0325\n",
            "Epoch 2 Batch 100 Val. Loss 0.0229\n",
            "Epoch 2 Batch 200 Val. Loss 0.0076\n",
            "Epoch 2 Batch 300 Val. Loss 0.0272\n",
            "Epoch 2 Batch 400 Val. Loss 0.0168\n",
            "Epoch 2 Batch 500 Val. Loss 0.0226\n",
            "Epoch 2 Batch 600 Val. Loss 0.0316\n",
            "Epoch 2 Loss 0.0264 -- Train Acc. 93.0675 -- Val Acc. 92.7484\n",
            "Time taken for 1 epoch 31.039151191711426 sec\n",
            "\n",
            "Epoch 3 Batch 0 Val. Loss 0.0197\n",
            "Epoch 3 Batch 100 Val. Loss 0.0343\n",
            "Epoch 3 Batch 200 Val. Loss 0.0300\n",
            "Epoch 3 Batch 300 Val. Loss 0.0167\n",
            "Epoch 3 Batch 400 Val. Loss 0.0323\n",
            "Epoch 3 Batch 500 Val. Loss 0.0292\n",
            "Epoch 3 Batch 600 Val. Loss 0.0246\n",
            "Epoch 3 Loss 0.0204 -- Train Acc. 94.1650 -- Val Acc. 93.3494\n",
            "Time taken for 1 epoch 30.90171194076538 sec\n",
            "\n",
            "Epoch 4 Batch 0 Val. Loss 0.0132\n",
            "Epoch 4 Batch 100 Val. Loss 0.0412\n",
            "Epoch 4 Batch 200 Val. Loss 0.0120\n",
            "Epoch 4 Batch 300 Val. Loss 0.0114\n",
            "Epoch 4 Batch 400 Val. Loss 0.0225\n",
            "Epoch 4 Batch 500 Val. Loss 0.0164\n",
            "Epoch 4 Batch 600 Val. Loss 0.0168\n",
            "Epoch 4 Loss 0.0176 -- Train Acc. 95.1025 -- Val Acc. 93.3293\n",
            "Time taken for 1 epoch 31.276707649230957 sec\n",
            "\n",
            "Epoch 5 Batch 0 Val. Loss 0.0016\n",
            "Epoch 5 Batch 100 Val. Loss 0.0038\n",
            "Epoch 5 Batch 200 Val. Loss 0.0295\n",
            "Epoch 5 Batch 300 Val. Loss 0.0187\n",
            "Epoch 5 Batch 400 Val. Loss 0.0172\n",
            "Epoch 5 Batch 500 Val. Loss 0.0043\n",
            "Epoch 5 Batch 600 Val. Loss 0.0069\n",
            "Epoch 5 Loss 0.0163 -- Train Acc. 95.3950 -- Val Acc. 92.7484\n",
            "Time taken for 1 epoch 31.56766653060913 sec\n",
            "\n",
            "Epoch 6 Batch 0 Val. Loss 0.0163\n",
            "Epoch 6 Batch 100 Val. Loss 0.0052\n",
            "Epoch 6 Batch 200 Val. Loss 0.0151\n",
            "Epoch 6 Batch 300 Val. Loss 0.0143\n",
            "Epoch 6 Batch 400 Val. Loss 0.0143\n",
            "Epoch 6 Batch 500 Val. Loss 0.0066\n",
            "Epoch 6 Batch 600 Val. Loss 0.0023\n",
            "Epoch 6 Loss 0.0140 -- Train Acc. 96.3225 -- Val Acc. 92.7484\n",
            "Time taken for 1 epoch 31.68458127975464 sec\n",
            "\n",
            "Epoch 7 Batch 0 Val. Loss 0.0064\n",
            "Epoch 7 Batch 100 Val. Loss 0.0130\n",
            "Epoch 7 Batch 200 Val. Loss 0.0021\n",
            "Epoch 7 Batch 300 Val. Loss 0.0061\n",
            "Epoch 7 Batch 400 Val. Loss 0.0101\n",
            "Epoch 7 Batch 500 Val. Loss 0.0061\n",
            "Epoch 7 Batch 600 Val. Loss 0.0118\n",
            "Epoch 7 Loss 0.0119 -- Train Acc. 97.1125 -- Val Acc. 92.9287\n",
            "Time taken for 1 epoch 31.663572311401367 sec\n",
            "\n",
            "Epoch 8 Batch 0 Val. Loss 0.0054\n",
            "Epoch 8 Batch 100 Val. Loss 0.0130\n",
            "Epoch 8 Batch 200 Val. Loss 0.0079\n",
            "Epoch 8 Batch 300 Val. Loss 0.0004\n",
            "Epoch 8 Batch 400 Val. Loss 0.0021\n",
            "Epoch 8 Batch 500 Val. Loss 0.0135\n",
            "Epoch 8 Batch 600 Val. Loss 0.0234\n",
            "Epoch 8 Loss 0.0106 -- Train Acc. 97.4175 -- Val Acc. 92.6482\n",
            "Time taken for 1 epoch 31.86322283744812 sec\n",
            "\n",
            "Epoch 9 Batch 0 Val. Loss 0.0051\n",
            "Epoch 9 Batch 100 Val. Loss 0.0101\n",
            "Epoch 9 Batch 200 Val. Loss 0.0024\n",
            "Epoch 9 Batch 300 Val. Loss 0.0109\n",
            "Epoch 9 Batch 400 Val. Loss 0.0119\n",
            "Epoch 9 Batch 500 Val. Loss 0.0018\n",
            "Epoch 9 Batch 600 Val. Loss 0.0123\n",
            "Epoch 9 Loss 0.0102 -- Train Acc. 97.6600 -- Val Acc. 92.7885\n",
            "Time taken for 1 epoch 32.24120211601257 sec\n",
            "\n",
            "Epoch 10 Batch 0 Val. Loss 0.0098\n",
            "Epoch 10 Batch 100 Val. Loss 0.0167\n",
            "Epoch 10 Batch 200 Val. Loss 0.0038\n",
            "Epoch 10 Batch 300 Val. Loss 0.0003\n",
            "Epoch 10 Batch 400 Val. Loss 0.0050\n",
            "Epoch 10 Batch 500 Val. Loss 0.0042\n",
            "Epoch 10 Batch 600 Val. Loss 0.0047\n",
            "Epoch 10 Loss 0.0088 -- Train Acc. 98.0800 -- Val Acc. 92.5681\n",
            "Time taken for 1 epoch 31.998352766036987 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnAM43gri7LJ",
        "colab_type": "text"
      },
      "source": [
        "### Stopping the Model\n",
        "\n",
        "How do we know when to stop the model. We can use a technique called `early stopping`, not covered here, but widely used in deep learning, to control the convergence of models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7w1P3PX9Qjq",
        "colab_type": "text"
      },
      "source": [
        "### Store the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvcWh9S9PHQ",
        "colab_type": "code",
        "outputId": "272c3df9-acc7-440f-a6db-33f3c6b6e36d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "torch.save(model, \"/gdrive/My Drive/NLP_PyTorch/emogru\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type EmoGRU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cB1oo9_cnN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "###Implementing more deep learning model\n",
        "Implement a model similar to the one above. Try to use an LSTM instead of an GRU. Go into the pytorch documentation and research quick ways to improve the model, like adding a `Dropout` [layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html). Anything to make your model faster and better. Also, add additional layers (i.e., make it deeper) to improve the model potential.\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}